# -*- coding: utf-8 -*-
"""Cryptocurrency_dataset_scrapping.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pVFiZzCYFWXODNzgaKAG7RjsVzA35riz
"""

pip install yfinance

import yfinance as yf

# Download Cardano data
cardano_data = yf.download("ADA-USD")

# Print column names
print(cardano_data.columns)







pip install requests beautifulsoup4

import yfinance as yf
import pandas as pd

# Define the Cardano ticker symbol on Yahoo Finance
ticker_symbol = "ADA-USD"

# Download historical data for Cardano
cardano_data = yf.download(ticker_symbol, start="2020-01-01", end="2023-01-01")

# Display the first few rows of the dataset
print(cardano_data.head())

# Save data to a CSV file
cardano_data.to_csv("cardano_data.csv")
print(cardano_data.columns)

import yfinance as yf
import pandas as pd
import datetime
import os

# Define the ticker and the interval
ticker_symbol = "ADA-USD"
interval = "1h"  # '1h' for hourly data; change to '1m' for minute-by-minute if needed

# Start and end dates for downloading
start_date = datetime.datetime(2021, 1, 1)  # Adjust as needed
end_date = datetime.datetime(2023, 1, 1)

# CSV file to store the data
output_file = "cardano_data.csv"

# Initialize an empty DataFrame to accumulate data
all_data = pd.DataFrame()

# Loop over date ranges in chunks
current_date = start_date
while current_date < end_date:
    next_date = current_date + datetime.timedelta(days=360)  # Fetch 7 days of data at a time

    # Download data for the date range
    cardano_data = yf.download(
        ticker_symbol,
        start=current_date.strftime("%Y-%m-%d"),
        end=next_date.strftime("%Y-%m-%d"),
        interval=interval
    )

    # Append to all_data DataFrame
    all_data = pd.concat([all_data, cardano_data])

    # Update the current date to the next date
    current_date = next_date

    # Check if the accumulated data is at least 10MB in size
    if all_data.memory_usage(index=True).sum() >= 10 * 1024 * 1024:  # 10MB
        break

# Save the data to CSV
all_data.to_csv(output_file)
print(f"Data saved to {output_file} with a size of {os.path.getsize(output_file) / (1024 * 1024):.2f} MB")



import yfinance as yf
import pandas as pd
import datetime
import os

# Define the ticker and the interval
ticker_symbol = "ADA-USD"
interval = "1h"  # '1h' for hourly data; change to '1m' for minute-by-minute if needed

# Start and end dates for downloading
start_date = datetime.datetime(2023, 1, 1)
end_date = datetime.datetime(2024, 1, 1)

# CSV file to store the data
output_file = "cardano_data2.csv"

# Initialize an empty DataFrame to accumulate data
all_data = pd.DataFrame()

# Loop over date ranges in chunks
current_date = start_date
while current_date < end_date:
    next_date = current_date + datetime.timedelta(days=8)  # Fetch 8 days of data at a time

    # Download data for the date range
    cardano_data = yf.download(
        ticker_symbol,
        start=current_date.strftime("%Y-%m-%d"),
        end=next_date.strftime("%Y-%m-%d"),
        interval=interval
    )

    # Append to all_data DataFrame
    all_data = pd.concat([all_data, cardano_data])

    # Update the current date to the next date
    current_date = next_date

    # Check if the accumulated data is at least 10MB in size
    if all_data.memory_usage(index=True).sum() >= 10 * 1024 * 1024:  # 10MB
        break

# Save the data to CSV
all_data.to_csv(output_file)
print(f"Data saved to {output_file} with a size of {os.path.getsize(output_file) / (1024 * 1024):.2f} MB")

import yfinance as yf
import pandas as pd
import datetime
import os

# Define the ticker and the interval
ticker_symbol = "ADA-USD"
interval = "1h"  # '1h' for hourly data; change to '1m' for minute-by-minute if needed

# Start and end dates for downloading
start_date = datetime.datetime(2017, 1, 1)  # Adjust as needed
end_date = datetime.datetime(2020, 1, 1)

# CSV file to store the data
output_file = "cardano_data3.csv"

# Initialize an empty DataFrame to accumulate data
all_data = pd.DataFrame()

# Loop over date ranges in chunks
current_date = start_date
while current_date < end_date:
    next_date = current_date + datetime.timedelta(days=10)  # Fetch 4 days of data at a time

    # Download data for the date range
    cardano_data = yf.download(
        ticker_symbol,
        start=current_date.strftime("%Y-%m-%d"),
        end=next_date.strftime("%Y-%m-%d"),
        interval=interval
    )

    # Append to all_data DataFrame
    all_data = pd.concat([all_data, cardano_data])

    # Update the current date to the next date
    current_date = next_date

    # Check if the accumulated data is at least 10MB in size
    if all_data.memory_usage(index=True).sum() >= 10 * 1024 * 1024:  # 10MB
        break

# Save the data to CSV
all_data.to_csv(output_file)
print(f"Data saved to {output_file} with a size of {os.path.getsize(output_file) / (1024 * 1024):.2f} MB")

import yfinance as yf

# Initialize the Bitcoin Cash ticker symbol
# Note: "BCH-USD" is typically the ticker for Bitcoin Cash in USD on yfinance
bch = yf.Ticker("BCH-USD")

# Download historical data (by default, this will include columns like 'Open', 'High', 'Low', 'Close', etc.)
data = bch.history(period="max")

# Print column names
print(data.columns)

import yfinance as yf
import pandas as pd
from datetime import datetime, timedelta

# Initialize Bitcoin Cash ticker
bch = yf.Ticker("BCH-USD")

# Define a function to collect data in chunks
def get_intraday_data(ticker, interval, days, chunk_size):
    all_data = []  # To store data chunks
    end_date = datetime.today()

    # Loop backwards in time to collect multiple chunks
    for _ in range(days // chunk_size):
        # Calculate start date for the current chunk
        start_date = end_date - timedelta(days=chunk_size)

        # Fetch historical data for the chunk
        data = ticker.history(start=start_date, end=end_date, interval=interval)
        all_data.append(data)

        # Update end_date for next iteration
        end_date = start_date

    # Concatenate all chunks into a single DataFrame
    full_data = pd.concat(all_data)
    return full_data

# Collect data in chunks of 60 days to get more granular data (1-minute interval)
data = get_intraday_data(bch, interval="5m", days=180, chunk_size=60)

# Print column names and preview data
print("Columns:", data.columns)
print(data.head())

# Save the data to a CSV file
data.to_csv("bitcoin_cash_intraday_data.csv")

# Check the size of the saved CSV file
file_size = round((data.memory_usage(deep=True).sum()) / (1024 ** 2), 2)  # Size in MB
print(f"Estimated dataset size: {file_size} MB")

import pandas as pd
import requests
from io import StringIO

# Define the URL for Cardano's hourly data on Binance from CryptoDataDownload
url = "https://www.cryptodatadownload.com/cdd/Binance_ADAUSDT_1h.csv"

# Send an HTTP request to fetch the CSV data
response = requests.get(url)

# Check if the request was successful
if response.status_code == 200:
    # Convert the content to a CSV-formatted DataFrame
    data = pd.read_csv(StringIO(response.content.decode("utf-8")), skiprows=1)

    # Display the column names and first few rows of the DataFrame
    print("Columns:", data.columns)
    print(data.head())

    # Optional: Save the dataset to a CSV file locally
    data.to_csv("cardano_data_hourly2.csv", index=False)

    # Check the file size of the saved CSV
    file_size_mb = data.memory_usage(deep=True).sum() / (1024 ** 2)
    print(f"Estimated dataset size: {file_size_mb:.2f} MB")
else:
    print(f"Failed to fetch data: HTTP {response.status_code}")

import pandas as pd
import requests
from io import StringIO

# List of URLs for different Cardano datasets (from various exchanges and intervals)
urls = [
    "https://www.cryptodatadownload.com/cdd/Binance_ADAUSDT_1m.csv",
    "https://www.cryptodatadownload.com/cdd/Binance_ADAUSDT_5m.csv",
    "https://www.cryptodatadownload.com/cdd/Binance_ADAUSDT_1h.csv",
    "https://www.cryptodatadownload.com/cdd/Bitfinex_ADAUSD_1h.csv",
    "https://www.cryptodatadownload.com/cdd/Bitfinex_ADAUSD_15m.csv",
    # Add more URLs if needed
]

# Initialize an empty list to store DataFrames
data_frames = []

# Download and load each dataset
for url in urls:
    response = requests.get(url)
    if response.status_code == 200:
        # Convert CSV content to DataFrame
        df = pd.read_csv(StringIO(response.content.decode("utf-8")), skiprows=1)
        data_frames.append(df)
        print(f"Downloaded data from {url}")
    else:
        print(f"Failed to fetch data from {url}")

# Concatenate all DataFrames into a single DataFrame
combined_data = pd.concat(data_frames, ignore_index=True)

# Sort data by date if the timestamp is in reverse chronological order
combined_data = combined_data.sort_values(by="date")  # adjust if 'date' has a different column name

# Save the combined dataset to a CSV file
combined_data.to_csv("cardano_combined_data.csv", index=False)

# Check the size of the saved CSV file
file_size_mb = combined_data.memory_usage(deep=True).sum() / (1024 ** 2)
print(f"Combined dataset size: {file_size_mb:.2f} MB")

# Step 1: Mount Google Drive if your file is stored there, or upload directly in Colab
# from google.colab import drive
# drive.mount('/content/drive')

import pandas as pd

# Step 2: Load the CSV file
file_path = '/content/cardano_combined_data.csv'  # Update this if your file path is different
data = pd.read_csv(file_path)

# Step 3: Check the columns in your dataset
print("Columns in the dataset:", data.columns)

# Step 4: Reorder columns (assuming the columns exist with these names)
ordered_columns = ['date', 'open', 'high', 'low', 'close', 'unix']
data = data[ordered_columns]

# Step 5: Sort by 'date' if needed, and display the first few rows
data = data.sort_values(by='date')
data.head()





import pandas as pd

# Step 1: Load the CSV file
file_path = '/content/cardano_combined_data.csv'  # Update if file path is different
data = pd.read_csv(file_path)

# Step 2: Reorder the columns and sort by 'date'
# Make sure these column names match exactly with your CSV file's headers
ordered_columns = ['date', 'open', 'high', 'low', 'close', 'unix']
data = data[ordered_columns].sort_values(by='date')

# Step 3: Save the reordered data to a new CSV file
output_path = 'cardano_combined_data_ordered.csv'  # Define your output file path
data.to_csv(output_path, index=False)

print("File saved as:", output_path)

import pandas as pd
import os

# Load the CSV files into DataFrames
df1 = pd.read_csv('file1.csv')  # Replace with your file path
df2 = pd.read_csv('file2.csv')  # Replace with your file path

# Merge the DataFrames on a common column (e.g., 'Date'), modify as needed
merged_df = pd.merge(df1, df2, on='Date', how='inner')  # Adjust the column name and merge type

# Sort the merged DataFrame by the 'Date' column (or any other column you need)
merged_df = merged_df.sort_values(by='Date')

# Save the merged and sorted DataFrame to a new CSV file
merged_df.to_csv('combined_sorted.csv', index=False)

# Calculate the file size of the new CSV in MB
file_size = os.path.getsize('combined_sorted.csv') / (1024 * 1024)  # Convert from bytes to MB
print(f"The size of the combined CSV is: {file_size:.2f} MB")

import os

# Define the file path (e.g., CSV, Excel, etc.)
file_path = '/content/cardano_combined_data_ordered.csv'  # Replace with your actual file path

# Get the size of the dataset file in bytes
file_size_bytes = os.path.getsize(file_path)

# Convert the size from bytes to kilobytes (KB) and megabytes (MB)
file_size_kb = file_size_bytes / 1024  # Convert to KB
file_size_mb = file_size_bytes / (1024 * 1024)  # Convert to MB

# Print the sizes
print(f"File Size: {file_size_bytes} bytes")
print(f"File Size: {file_size_kb:.2f} KB")
print(f"File Size: {file_size_mb:.2f} MB")

import pandas as pd
import requests
from io import BytesIO

# List of URLs for the Excel files
urls = [
    "https://www.cryptodatadownload.com/cdd/Binance_ADAUSDT_1h.csv",
    "https://www.cryptodatadownload.com/cdd/Bitfinex_ADAUSD_1h.csv",  # Replace with your Excel file URL
    # Replace with your Excel file URL
    # Add more URLs as needed
]

# Loop through the URLs to download and convert Excel files
for url in urls:
    # Send a GET request to download the Excel file
    response = requests.get(url)
    if response.status_code == 200:
        # Read the content of the response as an Excel file
        excel_file = BytesIO(response.content)

        # Load the Excel file into a DataFrame (this assumes the first sheet by default)
        df = pd.read_excel(excel_file)

        # Generate the CSV file name (replace .xlsx with .csv)
        csv_filename = url.split("/")[-1].replace(".xlsx", ".csv")

        # Save the DataFrame as a CSV file
        df.to_csv(csv_filename, index=False)
        print(f"Downloaded and saved {csv_filename}")
    else:
        print(f"Failed to fetch data from {url}")

import pandas as pd
import requests
from io import BytesIO

# List of URLs for the Excel files
urls = [
    "https://www.cryptodatadownload.com/cdd/Binance_ADAUSDT_1h.csv",
    "https://www.cryptodatadownload.com/cdd/Bitfinex_ADAUSD_1h.csv", # Replace with your Excel file URL
    # Add more URLs as needed
]

# Loop through the URLs to download and convert Excel files
for url in urls:
    # Send a GET request to download the Excel file
    response = requests.get(url)
    if response.status_code == 200:
        # Read the content of the response as an Excel file
        excel_file = BytesIO(response.content)

        try:
            # Load the Excel file into a DataFrame (specify engine for compatibility)
            df = pd.read_excel(excel_file, engine='openpyxl')  # Specify 'openpyxl' for .xlsx files
            # If the file is .xls, you can specify 'xlrd' as the engine:
            # df = pd.read_excel(excel_file, engine='xlrd')  # Uncomment if the file is in .xls format

            # Generate the CSV file name (replace .xlsx with .csv)
            csv_filename = url.split("/")[-1].replace(".xlsx", ".csv")

            # Save the DataFrame as a CSV file
            df.to_csv(csv_filename, index=False)
            print(f"Downloaded and saved {csv_filename}")

        except Exception as e:
            print(f"Failed to read or convert {url}: {e}")
    else:
        print(f"Failed to fetch data from {url}")

import pandas as pd
import requests
from io import StringIO

# List of URLs for the CSV files (not Excel files)
urls = [
    "https://www.cryptodatadownload.com/cdd/Binance_ADAUSDT_1h.csv",  # Replace with your CSV file URL
    "https://www.cryptodatadownload.com/cdd/Bitfinex_ADAUSD_1h.csv",  # Replace with your CSV file URL
    # Add more URLs as needed
]

# Loop through the URLs to download and convert CSV files
for url in urls:
    # Send a GET request to download the CSV file
    response = requests.get(url)
    if response.status_code == 200:
        # Read the content of the response as CSV using StringIO
        csv_file = StringIO(response.content.decode('utf-8'))

        try:
            # Load the CSV file into a DataFrame
            df = pd.read_csv(csv_file)

            # Generate the CSV file name (no need to change the extension as it's already CSV)
            csv_filename = url.split("/")[-1]

            # Save the DataFrame as a CSV file
            df.to_csv(csv_filename, index=False)
            print(f"Downloaded and saved {csv_filename}")

        except Exception as e:
            print(f"Failed to read or convert {url}: {e}")
    else:
        print(f"Failed to fetch data from {url}")

import pandas as pd

# Load the two CSV files
file1 = pd.read_csv('/content/Binance_ADAUSDT_1h.csv')  # Replace with your first CSV file path
file2 = pd.read_csv('/content/Bitfinex_ADAUSD_1h.csv')  # Replace with your second CSV file path

# Check column names to ensure the common column exists
print(file1.columns)
print(file2.columns)

# Merge the dataframes (Assuming the common column is 'date')
merged_data = pd.merge(file1, file2, on='date', how='outer')  # Replace 'date' with the correct column name

# Save the merged data to a new CSV file
merged_data.to_csv('merged_data.csv', index=False)

print("The CSV files have been successfully merged and saved as 'merged_data.csv'")

import pandas as pd
import requests

# URL of the merged CSV file (replace this with the actual URL of your merged file)
merged_csv_url = "https://example.com/path/to/your/merged_file.csv"

# Send a GET request to download the merged CSV file
response = requests.get(merged_csv_url)

# Check if the request was successful
if response.status_code == 200:
    # Save the CSV file locally
    with open("merged_cardano_data.csv", "wb") as file:
        file.write(response.content)
    print("Merged CSV file has been downloaded and saved as 'merged_cardano_data.csv'")
else:
    print(f"Failed to download the merged CSV file. Status code: {response.status_code}")

pip install requests pandas beautifulsoup4



import pandas as pd
import requests
from io import StringIO

# URL of the cryptocurrency dataset (example from Binance ADAUSDT data)
url = "https://www.cryptodatadownload.com/cdd/Binance_ADAUSDT_1h.csv"

# Send GET request to fetch the CSV file
response = requests.get(url)
if response.status_code == 200:
    # Check if the content seems to be valid CSV by printing the first few bytes
    print(response.content[:500])  # Inspect the first few bytes

    # If it seems like a CSV, you can proceed
    # If not, further inspection of the raw file is needed

    # Load the CSV data into a pandas DataFrame
    csv_data = StringIO(response.content.decode('utf-8'))
    df = pd.read_csv(csv_data)

    # Print column names to ensure correct format
    print("Columns in dataset:", df.columns)

    # Clean up column names (strip extra spaces)
    df.columns = df.columns.str.strip()

    # Inspect the first few rows of the DataFrame to confirm correct data
    print(df.head())

    # Check if the 'Unix' or 'Date' column is there
    if 'Unix' in df.columns:
        df['date'] = pd.to_datetime(df['Unix'], unit='ms')  # Convert Unix timestamp
    elif 'Date' in df.columns:
        df['date'] = pd.to_datetime(df['Date'])  # If it's a Date column

    # Filter data based on valid date
    df_filtered = df[df['date'].notnull()]

    # Optionally, you can limit the number of rows (based on the 25MB dataset size requirement)
    df_filtered = df_filtered.head(10000)  # or any other limit based on file size

    # Save the filtered data to a new CSV file
    df_filtered.to_csv('cryptocurrency_data_filtered.csv', index=False)

    print(f"Dataset successfully saved as 'cryptocurrency_data_filtered.csv'")
else:
    print(f"Failed to fetch data. Status code: {response.status_code}")

import pandas as pd
import requests
from io import StringIO

# URL of the cryptocurrency dataset (example from Binance ADAUSDT data)
url = "https://www.cryptodatadownload.com/cdd/Binance_ADAUSDT_1h.csv"

# Send GET request to fetch the CSV file
response = requests.get(url)
if response.status_code == 200:
    # Check the first few bytes to verify content and avoid the URL line
    print(response.content[:500])  # Inspect the first 500 bytes to identify the issue

    # Load the CSV data into a pandas DataFrame, skipping the first row (URL line)
    csv_data = StringIO(response.content.decode('utf-8'))
    df = pd.read_csv(csv_data, skiprows=1)  # Skip the first row containing URL

    # Print column names to ensure correct format
    print("Columns in dataset:", df.columns)

    # Clean up column names (strip extra spaces)
    df.columns = df.columns.str.strip()

    # Inspect the first few rows of the DataFrame to confirm correct data
    print(df.head())

    # Convert the 'Unix' or 'Date' column (depending on the dataset) to datetime
    if 'Unix' in df.columns:
        df['date'] = pd.to_datetime(df['Unix'], unit='ms')  # Convert Unix timestamp
    elif 'Date' in df.columns:
        df['date'] = pd.to_datetime(df['Date'])  # If it's a Date column

    # Filter data based on valid date
    df_filtered = df[df['date'].notnull()]

    # Optionally, you can limit the number of rows (based on the 25MB dataset size requirement)
    df_filtered = df_filtered.head(10000)  # or any other limit based on file size

    # Save the filtered data to a new CSV file
    df_filtered.to_csv('cryptocurrency_data_filtered.csv', index=False)

    print(f"Dataset successfully saved as 'cryptocurrency_data_filtered.csv'")
else:
    print(f"Failed to fetch data. Status code: {response.status_code}")
# Assuming the DataFrame 'df_filtered' has been cleaned and prepared

# Specify the path where you want to save the CSV
output_file = 'cryptocurrency_data_filtered.csv'

# Save the DataFrame to CSV
df_filtered.to_csv(output_file, index=False)

print(f"The cleaned dataset has been successfully saved to {output_file}")



# After cleaning or processing your DataFrame (e.g., df_filtered)

# Specify the output file path and name
output_file = 'cryptocurrency_data_filtered.csv'

# Save the cleaned DataFrame to a CSV file
df_filtered.to_csv(output_file, index=False)

# Print a confirmation message
print(f"Cleaned dataset has been saved as '{output_file}'")
# Assuming df_filtered is the cleaned DataFrame
df_filtered.to_csv('cryptocurrency_data_filtered.csv', index=False)
from google.colab import files
files.download('cryptocurrency_data_filtered.csv')

pip install pycoingecko

import pandas as pd
from pycoingecko import CoinGeckoAPI
from time import time

# Initialize CoinGecko API client
cg = CoinGeckoAPI()

# Specify the cryptocurrency (e.g., Bitcoin)
crypto_id = 'cardano'  # You can change this to any other cryptocurrency (e.g., 'ethereum', 'bitcoin', etc.)
currency = 'usd'  # You can change the currency (e.g., 'usd', 'eur')

# Get the current Unix timestamp
current_time = int(time())  # Current time in seconds since epoch

# Set the "from" timestamp to 365 days ago
from_time = current_time - (365 * 24 * 60 * 60)  # 365 days in seconds

# Fetch data for the past 365 days
data = cg.get_coin_market_chart_range_by_id(id=crypto_id, vs_currency=currency, from_timestamp=from_time, to_timestamp=current_time)

# Convert the data into a pandas DataFrame
prices = data['prices']  # 'prices' contains the historical data

# Convert the timestamp to date-time format
df = pd.DataFrame(prices, columns=['timestamp', 'price'])
df['date'] = pd.to_datetime(df['timestamp'], unit='ms')  # Convert timestamp to datetime

# Drop the 'timestamp' column if you only want date, time, and price
df = df.drop(columns=['timestamp'])

# Save the dataframe to a CSV file
df.to_csv('coin_gecko_data.csv', index=False)

# Check the file size
file_size = df.memory_usage(deep=True).sum() / (1024**2)  # Size in MB
print(f"Data size: {file_size:.2f} MB")

import pandas as pd
from pycoingecko import CoinGeckoAPI
from time import time

# Initialize CoinGecko API client
cg = CoinGeckoAPI()

# List of cryptocurrencies (You can add more coins to increase the data size)
cryptos = ['cardano', 'bitcoin', 'ripple']
currency = 'usd'

# Get the current Unix timestamp
current_time = int(time())

# Set the "from" timestamp to 365 days ago
from_time = current_time - (365 * 24 * 60 * 60)  # 365 days in seconds

# Initialize an empty list to store data
all_data = []

# Fetch data for each cryptocurrency
for crypto_id in cryptos:
    data = cg.get_coin_market_chart_range_by_id(id=crypto_id, vs_currency=currency, from_timestamp=from_time, to_timestamp=current_time)
    prices = data['prices']  # 'prices' contains the historical data

    # Convert the data into a pandas DataFrame
    df = pd.DataFrame(prices, columns=['timestamp', f'{crypto_id}_price'])
    df['date'] = pd.to_datetime(df['timestamp'], unit='ms')  # Convert timestamp to datetime
    df = df.drop(columns=['timestamp'])

    # Append the data
    all_data.append(df)

# Combine the data for all cryptocurrencies into one DataFrame
combined_df = pd.concat(all_data, axis=1)

# Remove duplicate 'date' column
combined_df = combined_df.loc[:, ~combined_df.columns.duplicated()]

# Save the dataframe to a CSV file
combined_df.to_csv('coin_gecko_25mb_data.csv', index=False)

# Check the file size
file_size = combined_df.memory_usage(deep=True).sum() / (1024**2)  # Size in MB
print(f"Data size: {file_size:.2f} MB")

import pandas as pd
from pycoingecko import CoinGeckoAPI
from time import time

# Initialize CoinGecko API client
cg = CoinGeckoAPI()

# List of cryptocurrencies to increase data size
cryptos = ['cardano', 'bitcoin', 'ethereum', 'ripple', 'litecoin']  # Add more coins as needed
currency = 'usd'

# Get the current Unix timestamp
current_time = int(time())

# Set the "from" timestamp to 365 days ago
from_time = current_time - (365 * 24 * 60 * 60)  # 365 days in seconds

# Initialize an empty list to store data
all_data = []

# Fetch data for each cryptocurrency
for crypto_id in cryptos:
    data = cg.get_coin_market_chart_range_by_id(id=crypto_id, vs_currency=currency, from_timestamp=from_time, to_timestamp=current_time)
    prices = data['prices']  # 'prices' contains the historical data

    # Convert the data into a pandas DataFrame
    df = pd.DataFrame(prices, columns=['timestamp', f'{crypto_id}_price'])
    df['date'] = pd.to_datetime(df['timestamp'], unit='ms')  # Convert timestamp to datetime
    df = df.drop(columns=['timestamp'])

    # Append the data
    all_data.append(df)

# Combine the data for all cryptocurrencies into one DataFrame
combined_df = pd.concat(all_data, axis=1)

# Remove duplicate 'date' column
combined_df = combined_df.loc[:, ~combined_df.columns.duplicated()]

# Save the dataframe to a CSV file
combined_df.to_csv('coin_gecko_15mb_data.csv', index=False)

# Check the file size
file_size = combined_df.memory_usage(deep=True).sum() / (1024**2)  # Size in MB
print(f"Data size: {file_size:.2f} MB")

import requests
import pandas as pd
from time import time

# Function to fetch historical data from CryptoCompare API
def fetch_data(crypto_symbol, to_symbol, limit=2000, to_timestamp=None, freq='hour'):
    url = f'https://min-api.cryptocompare.com/data/v2/histohour' if freq == 'hour' else f'https://min-api.cryptocompare.com/data/v2/histominute'

    parameters = {
        'fsym': crypto_symbol,
        'tsym': to_symbol,
        'limit': limit,  # number of data points (max 2000 per call)
        'toTs': to_timestamp,  # timestamp to specify the range end
    }
    response = requests.get(url, params=parameters)
    data = response.json()
    return data['Data']['Data']

# Specify the coin and time range (let's go for 5 years in this case)
crypto_symbol = 'ADA'  # You can change this to 'BTC', 'ETH', etc.
to_symbol = 'USD'
to_timestamp = int(time())  # Current timestamp
from_timestamp = to_timestamp - (5 * 365 * 24 * 60 * 60)  # 5 years in seconds (365 days/year)

# Fetch data for multiple coins or extend the range
all_data = []

# Fetch data for the coin in chunks
while to_timestamp > from_timestamp:
    data = fetch_data(crypto_symbol, to_symbol, to_timestamp=to_timestamp, freq='hour')
    all_data.extend(data)
    to_timestamp = data[-1]['time'] - 1  # Move to previous time chunk

# Create DataFrame from fetched data
df = pd.DataFrame(all_data)
df['date'] = pd.to_datetime(df['time'], unit='s')  # Convert to dat

import pandas as pd

# Load the CSV files
df1 = pd.read_csv('D:\Downloads\cardano_data_reordered.csv')
df2 = pd.read_csv('D:\Downloads\Bitfinex_ADAUSD_1h.csv')

# Merge the datasets on a common column
# Replace 'common_column' with the actual name of the column you want to merge on
merged_df = pd.merge(df1, df2, on='common_column', how='inner')  # 'inner' merge keeps only matching rows

# Save the merged data to a new CSV file
merged_df.to_csv('merged_output.csv', index=False)
print("Files have been merged and saved as 'merged_output.csv'")

import pandas as pd

# Load your datasets
df1 = pd.read_csv('/content/Binance_ADAUSDT_1h - Copy.csv')
df2 = pd.read_csv('/content/Bitfinex_ADAUSD_1h.csv')
df3 = pd.read_csv('/content/cardano_combined_data_ordered.csv')
df4 = pd.read_csv('/content/cardano_data_reordered_updated (1).csv')

# Merge the datasets (you can adjust the merge strategy as needed)
# Example: Concatenating them vertically (i.e., stacking them one below the other)
merged_df = pd.concat([df1, df2, df3, df4], ignore_index=True)

# Save the merged dataset to a single CSV file
merged_df.to_csv('merged_dataset.csv', index=False)

print("Datasets merged and saved successfully.")
from google.colab import files

# Download the file to local machine
files.download('merged_dataset.csv')